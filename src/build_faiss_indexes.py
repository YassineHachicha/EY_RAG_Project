import os
import json
import faiss
import numpy as np
import textwrap
from pathlib import Path
from sentence_transformers import SentenceTransformer

# ğŸ“ RÃ©pertoires
documents_dir = Path("documents")
index_dir = Path("faiss_index")
index_dir.mkdir(exist_ok=True)

# ğŸ§  ModÃ¨le d'embedding
model = SentenceTransformer("all-MiniLM-L6-v2")

# ğŸ“¦ Mapping pour chunks
chunk_mapping = {}

# ğŸ” Pour chaque document
for file_path in documents_dir.glob("*.txt"):
    doc_name = file_path.stem  # Exemple : basel3
    print(f"ğŸ“„ Traitement de {doc_name}.txt")

    with open(file_path, "r", encoding="utf-8") as f:
        text = f.read()

    # âœ‚ï¸ DÃ©couper le texte (~300 caractÃ¨res)
    chunks = textwrap.wrap(text, width=300)
    chunk_mapping[doc_name] = chunks

    # ğŸ”¢ Embedding des chunks
    embeddings = model.encode(chunks)
    embedding_matrix = np.array(embeddings).astype("float32")

    # ğŸ§Š CrÃ©ation de lâ€™index FAISS
    dim = embedding_matrix.shape[1]
    index = faiss.IndexFlatL2(dim)
    index.add(embedding_matrix)

    # ğŸ’¾ Sauvegarde de l'index
    faiss.write_index(index, str(index_dir / f"{doc_name}.idx"))


# ğŸ’¾ Sauvegarder le mapping chunks â†” documents
with open(index_dir / "chunk_mapping.json", "w", encoding="utf-8") as f:
    json.dump(chunk_mapping, f, indent=2)

print("âœ… Indexation terminÃ©e.")
