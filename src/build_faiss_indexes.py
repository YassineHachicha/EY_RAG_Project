
import os
import json
import faiss
import numpy as np
import textwrap
from pathlib import Path
from sentence_transformers import SentenceTransformer

# ğŸ“ RÃ©pertoires
documents_dir = Path("documents")
index_dir = Path("faiss_index")
index_dir.mkdir(exist_ok=True)

# ğŸ§  ModÃ¨le d'embedding
model = SentenceTransformer("all-MiniLM-L6-v2")

# ğŸ“¦ Mapping pour chunks
chunk_mapping = {}

# ğŸ” Parcourir chaque .txt
for file_path in documents_dir.glob("*.txt"):
    doc_name = file_path.stem
    print(f"ğŸ“„ Indexation : {doc_name}.txt")

    with open(file_path, "r", encoding="utf-8") as f:
        text = f.read()

    # âœ‚ï¸ Chunking (500 caractÃ¨res)
    chunks = textwrap.wrap(text, width=500)
    chunk_mapping[doc_name] = chunks

    # ğŸ”¢ Embeddings
    embeddings = model.encode(chunks, show_progress_bar=False)
    embedding_matrix = np.array(embeddings).astype("float32")

    # ğŸ”„ FAISS index
    dim = embedding_matrix.shape[1]
    index = faiss.IndexFlatL2(dim)
    index.add(embedding_matrix)

    # ğŸ’¾ Sauvegarde
    faiss.write_index(index, str(index_dir / f"{doc_name}.idx"))

# ğŸ’¾ Mapping chunks â†” documents
with open(index_dir / "chunk_mapping.json", "w", encoding="utf-8") as f:
    json.dump(chunk_mapping, f, indent=2)

print("âœ… Indexation terminÃ©e pour tous les documents.")
